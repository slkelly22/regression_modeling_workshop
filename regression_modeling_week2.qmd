---
title: "Regression Modeling in R: Week 2"
author: "S.Kelly"
format: 
  revealjs: 
    theme: serif
    code-overflow: wrap
    footer: "S.Kelly | Regression Modeling in R | Spring 2026"
editor: visual
execute: 
  echo: true
---

## Returning to Last Week's Dataset

`Cassady` data from `Multilevel Modeling Using R` by Finch and Bolin. You can access the `Cassady` data from [www.mlminr.com.](https://www.mlminr.com/data-sets)

![](images/MLMBook.png)

## Let's read in the Cassady Data

```{r}
#| echo: TRUE
# install.packages("tidyverse")
library(tidyverse)
```

```{r}
#| echo: TRUE
students <- read_csv("data/cassady_finch_bolin.csv")
head(students, 4)
```

## Quick Look at the Data

```{r}
summary(students)
```

## Let's correct the data entry error

```{r}
students |> 
  filter(GPA > 5) 
```

## Let's correct the data entry error

Let's assume we checked and the GPA was 3.02, not 302.

```{r}
students_clean <- students |> 
  mutate(GPA = if_else(GPA > 5, 3.02, GPA))
```

<br>

```{r}
students_clean |> 
  filter(GPA > 5) 
```

## Outcome Variable

Our outcome variable is GPA. This is what we want to model. Let's take a quick look at the distribution.

```{r}
students_clean |> 
  ggplot(aes(x = GPA)) + geom_histogram() 
```

## Last Week's Multiple Regression

```{r}
model_gpa_anxiety <- lm(GPA ~ CTA.tot + BStotal, data = students_clean)
summary(model_gpa_anxiety)
```

## I want to see the *p*-value!

You can turn off the scientific notation

```{r}
options(scipen = 999)
```

Now run the model again

```{r}
summary(model_gpa_anxiety)
```

## How do we test an interaction in R?

> Two different ways to set up the syntax:

`variable1 + variable2 + variable1:variable2` explicitly requests main effects and the interaction

`variable1` \* `variable2` interaction term with implied main effects

## Let's add an interaction term

```{r}
model_gpa_anxiety2 <- lm(GPA ~ CTA.tot + BStotal + CTA.tot:BStotal, 
                         data = students_clean)
summary(model_gpa_anxiety2)
```

## Let's compare models

Is the model with the interaction term a better fit?

```{r}
anova(model_gpa_anxiety, model_gpa_anxiety2)
```

## What if it was significant?

When the interaction is significant, you need to understand the coefficients in the context of that interaction.

-   If categorical, create regression lines by group; easy to do in ggplot

-   If continuous, you can calculate the predicted values of the outcome at particular values in the predictor

-   For more details see the `interactions` and `emmeans` packages. We'll also return to this in Week 4.

## What about GPA by Gender?

```{r}
students_clean |> 
  ggplot(aes(x = GPA)) + geom_histogram() + 
  facet_wrap(~ Gender)
```

## Categorical Predictors

We used two continuous predictors last week, but how does R handle categorical predictors? Let's look at our data again.

```{r}
str(students_clean)
```

## Categorical Predictors

What does the gender variable currently look like?

```{r}
unique(students_clean$Gender)
```

Not technically a categorical variable, but does that matter in the binary case?

Let's just proceed and see what happens when we add this as a third variable to the model.

```{r}
model_gpa_anxiety_gender <- lm(GPA ~ CTA.tot + BStotal + Gender, 
                               data = students_clean)
```

## How to Interpret?

In order to interpret, you must understand the raw data!

```{r}
summary(model_gpa_anxiety_gender)
```

## Setting a Factor

Let's turn the `Gender` numeric variable into a factor so we can add labels and order

```{r}
students_clean$gender_factor <- factor(students_clean$Gender, 
                                 levels = c(1, 2), 
                                 labels = c("Female", "Male"))
```

Let's look at our new factor

```{r}
levels(students_clean$gender_factor)
```

```{r}
fct_count(students_clean$gender_factor)
```

## With the new factor...

```{r}
model_gpa_anxiety_gender2 <- lm(GPA ~ CTA.tot + BStotal + gender_factor, 
                               data = students_clean)
summary(model_gpa_anxiety_gender2)
```

## Am I sure? 

Checking the average GPA by gender

```{r}
students_clean |> 
  group_by(gender_factor) |> 
  summarize(gender_GPA = mean(GPA, na.rm = T))
```

## New variable with three levels

```{r}
students_clean <- students_clean |> 
  mutate(adult_group = case_when(
    Age <= 19 ~ "young",
    Age > 19 & Age <= 21 ~ "mid",
    Age > 21 ~ "older"))

# How many of each?
count(students_clean,adult_group)
```

## But it's not a factor...

```{r}
str(students_clean$adult_group)
```

## Let's model GPA anyway...

```{r}
GPA_adult <- lm(GPA ~ adult_group, data = students_clean)
summary(GPA_adult)
```

## What's our reference category?

```{r}
GPA_adult$coefficients
```

We need a factor to switch the reference category...

```{r}
students_clean$adult_fac <- factor(students_clean$adult_group, 
                             levels = c("young", "mid", "older"))
```

```{r}
levels(students_clean$adult_fac)
```

## Now let's run the model again

```{r}
GPA_adult2 <- lm(GPA ~ adult_fac, data = students_clean)
summary(GPA_adult2)
```

## Plotting with Factors

```{r}
students_clean |> 
  drop_na() |> 
  ggplot(aes(x = adult_fac, y = GPA)) + geom_boxplot(fill = "slateblue")
```

## Bonus Activity

You have access to the `gss_cat` data that lives inside the `forcats` package (part of tidyverse).

```{r}
summary(gss_cat)
```

## Bonus Activity

Create a linear model predicting `tvhours`. Use `age` and an additional factor (your choice) as predictors. Be sure to check the levels of your factor so you can interpret the coefficients.

```{r}
#| echo: TRUE
#| eval: FALSE
# check the levels
levels(gss_cat$ ? )

# create the model
tv_model <- lm( ? ~ ? + ? , data = ? )
summary(tv_model)
```

## Next Week

> We'll work through models with binary outcomes -- logistic regression!

## Upcoming Event: March 17th

![](images/GMAS_Event_Spring2026.png){fig-align="center"}
